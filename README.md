# Geometric LÃ©vy Dynamics and Criticality in Deep Learning

*LEVBOT is a unified information-geometric theory modeling deep learning as LÃ©vy-driven stochastic flow on curved manifolds, where heavy-tailed optimization noise and curvature jointly trigger phase transitions in representation learning*

---

## ğŸš€ Overview

**LEVBOT** presents a new theoretical framework for understanding deep learning dynamics as:

> **Stochastic flow on a curved statistical manifold driven by LÃ©vy (heavy-tailed) noise, where learning phase transitions emerge from joint noiseâ€“curvature criticality.**

Rather than modeling SGD as Gaussian diffusion in flat parameter space, LEVBOT treats training as **Î±-stable stochastic motion on the Fisherâ€“Rao information manifold**, capturing:

- heavy-tailed gradient noise  
- edge-of-stability dynamics  
- feature-learning transitions  
- grokking-style generalization jumps  
- curvature-driven amplification  

within one coherent dynamical system.

---

## ğŸ“‰ Why classical SGD theory breaks

Traditional analyses approximate SGD as Brownian motion in Euclidean space.

Modern deep networks violate this assumption:

- gradient noise is heavy-tailed  
- rare jumps dominate exploration  
- learning concentrates near instability boundaries  

LEVBOT replaces diffusion with **LÃ©vy-driven stochastic dynamics on curved information geometry**, aligning theory with empirical behavior.

---

## ğŸ§  Learning on a statistical manifold

Training evolves on:

\[
\mathcal{M} = \{ p(x \mid \theta(t)) \}
\]

equipped with the **Fisherâ€“Rao metric**:

\[
g_{ij}(t)=\mathbb{E}[\partial_{\theta_i}\log p \; \partial_{\theta_j}\log p]
\]

This measures **functional sensitivity of learned representations**, not raw parameter displacement.

---

## ğŸ“ˆ Temporal information density (learning leverage)

Define:

\[
\rho(t)=\mathrm{tr}\,g(t)
\]

### Interpretation

| Regime | Geometry | Learning behavior |
|-------|---------|------------------|
| Ï(t) â‰ˆ 0 | flat | lazy / NTK-like |
| high Ï(t) | sensitive | rapid feature formation |
| spikes | critical | phase transitions |

**Ï(t) objectively tracks where learning actually occurs.**

---

## âš¡ LÃ©vy dynamics on curved manifolds

SGD follows:

\[
d\theta_t = -\nabla L\,dt + \sigma\, dL_t^{(\alpha)}
\]

with **Î±-stable LÃ©vy noise (1 < Î± < 2)**.

Probability flow obeys the **fractional Fokkerâ€“Planck equation**:

\[
\partial_t p
= \nabla\cdot(p\nabla L)
+ D_\alpha (-\Delta_g)^{\alpha/2} p
\]

where:

- Î”_g is the Laplaceâ€“Beltrami operator on the Fisher manifold  
- jumps dominate exploration over diffusion  

---

## ğŸ“Š LÃ©vy-corrected consolidation ratio

\[
C_\alpha(t)=\frac{|\nabla L|^2}{2D_\alpha d}
\]

with:

\[
D_\alpha \propto s_\alpha^\alpha / B
\]

### Regimes

| CÎ± | Dynamics |
|---|---------|
| â‰«1 | deterministic descent |
| â‰ª1 | jump-dominated exploration |
| â‰ˆ1 | critical balance |

---

## ğŸŒ€ Curvature as amplification engine

Scalar curvature R(t) governs geodesic instability:

\[
\frac{D^2J}{dt^2}+R(J,\dot\gamma)\dot\gamma=0
\]

High curvature causes exponential trajectory separation, explaining:

- grokking jumps  
- sudden generalization  
- sharp-minimum instability  

as **geometric phase transitions**.

---

## ğŸ“ Joint criticality law (central prediction)

Learning transitions occur when:

\[
\boxed{
C_\alpha(t)\approx1
\quad\land\quad
\lambda_{\max}(H)\eta\approx2
\quad\land\quad
\rho(t)\ \text{peaks}
}
\]

| Term | Captures |
|-----|---------|
| CÎ± | noise vs signal |
| Î»max Î· | edge of stability |
| Ï(t) | representational sensitivity |

This unifies **stochasticity, geometry, and stability** into one dynamical condition.

---

## ğŸ” Feature learning as geometric phase transition

- lazy regime â†’ flat Fisher geometry  
- feature learning â†’ spectrum reorganization + curvature spikes  
- LÃ©vy jumps move between representation basins  

**Feature formation is a geometric transition, not optimizer magic.**

---

## ğŸ§ª Research directions

### Theory
- fractional Fokkerâ€“Planck on statistical manifolds  
- derivation of criticality conditions  
- curvature-driven generalization theory  

### Empirical
- track Ï(t) vs grokking  
- predict instability better than loss/sharpness  
- validate LÃ©vy scaling in modern networks  

### Algorithms
**Geometric LÃ©vy-adaptive optimizer**:

\[
\eta(t)\propto \frac{1}{\lambda_{\max}(H)} f(C_\alpha,\rho)
\]

---

## ğŸ“Œ Core contributions

- LÃ©vy dynamics on Fisher information geometry  
- temporal leverage density Ï(t)  
- curvature-driven phase transitions  
- unified noiseâ€“geometryâ€“stability law  
- bridge between optimization and representation learning  

---

## âš  Current limitations (transparent)

This framework is conceptually complete but mathematically open:

- fractional Laplaceâ€“Beltrami dynamics largely unproved  
- joint criticality presently heuristic  
- empirical validation in progress  

These define the active research frontier.





